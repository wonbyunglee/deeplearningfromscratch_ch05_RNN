"""
models/bilstm.py (PyTorch)
- BiLSTM íšŒê·€ ëª¨ë¸ (many-to-one)
- ì…ë ¥: ê³¼ê±° Lë…„ì¹˜ feature ì‹œí€€ìŠ¤ (B, L, F)
- ì¶œë ¥: ë‹¤ìŒ ì‹œì¦Œ ERA ì˜ˆì¸¡ (B, 1)

BiLSTM ê°œë…:
- ì–‘ë°©í–¥ RNN(Schuster & Paliwal, 1997) + LSTM ì…€(Hochreiter & Schmidhuber, 1997)
- ì •ë°©í–¥(ê³¼ê±° â†’ í˜„ì¬)ê³¼ ì—­ë°©í–¥(ë¯¸ë˜ â†’ í˜„ì¬) ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©
"""
from __future__ import annotations

import torch
import torch.nn as nn


class BiLSTMRegressor(nn.Module):
    def __init__(
        self,
        input_size: int,          # F: í•œ ì‹œì (í•œ ì‹œì¦Œ)ì— ë“¤ì–´ê°€ëŠ” feature ê°œìˆ˜
        hidden_size: int = 128,    # H: ë‹¨ë°©í–¥ LSTM ì€ë‹‰ ìƒíƒœ ì°¨ì›
        num_layers: int = 1,       # LSTM ì¸µ ìˆ˜(ìŠ¤íƒ LSTM)
        dropout: float = 0.3       # FC(head)ì—ì„œ ì‚¬ìš©í•  ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    ):
        super().__init__()

        # âœ… BiLSTM ë³¸ì²´
        # - ì…ë ¥: (B, L, F)
        # - ì¶œë ¥(out): (B, L, 2H)
        #   â†’ forward hidden(H) + backward hidden(H)ë¥¼ concat
        # - ì¶œë ¥(h_n): (num_layers*2, B, H)
        #   â†’ ê° layerì˜ forward / backward ë§ˆì§€ë§‰ timestep hidden
        # - ì¶œë ¥(c_n): (num_layers*2, B, H)
        #
        # bidirectional=True ì„¤ì •ìœ¼ë¡œ:
        # - ì •ë°©í–¥ LSTM: t = 1 â†’ L
        # - ì—­ë°©í–¥ LSTM: t = L â†’ 1
        # ë‘ ê°œê°€ ë™ì‹œì— ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•œë‹¤.
        self.bilstm = nn.LSTM(
            input_size=input_size,     # ì…ë ¥ feature ì°¨ì› F
            hidden_size=hidden_size,   # ë‹¨ë°©í–¥ ì€ë‹‰ ì°¨ì› H
            num_layers=num_layers,     # LSTM ì¸µ ìˆ˜
            batch_first=True,          # ì…ë ¥ì„ (B, L, F) í˜•íƒœë¡œ ë°›ìŒ
            bidirectional=True,        # ğŸ”‘ ì–‘ë°©í–¥ LSTM
        )

        # âœ… íšŒê·€(Regression) Head
        # - BiLSTMì˜ ì¶œë ¥ hidden ì°¨ì›ì€ 2Hì´ë¯€ë¡œ
        #   Linear ì…ë ¥ ì°¨ì›ë„ hidden_size * 2
        #
        # êµ¬ì„± ì˜ë„:
        # - Dropout: ê³¼ì í•© ë°©ì§€
        # - Linear(2H -> 64) + ReLU: ë¹„ì„ í˜• ë³€í™˜
        # - Linear(64 -> 1): ìµœì¢… ERA ì˜ˆì¸¡
        self.head = nn.Sequential(
            nn.Dropout(dropout),                # (B, 2H) ë“œë¡­ì•„ì›ƒ
            nn.Linear(hidden_size * 2, 64),     # (B, 2H) -> (B, 64)
            nn.ReLU(),                          # ë¹„ì„ í˜• í™œì„±í™”
            nn.Dropout(dropout),                # (B, 64) ë“œë¡­ì•„ì›ƒ
            nn.Linear(64, 1),                   # (B, 64) -> (B, 1)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, L, F)
        # - B: batch size
        # - L: sequence length (ê³¼ê±° ëª‡ ë…„ì¹˜ ê¸°ë¡; ì˜ˆ: 2/3/4)
        # - F: feature dimension (í•œ ì‹œì¦Œë‹¹ ë…ë¦½ë³€ìˆ˜ ê°œìˆ˜)
        out, (h_n, c_n) = self.bilstm(x)

        # out: (B, L, 2H)
        # - ê° timestepë§ˆë‹¤
        #   [ì •ë°©í–¥ hidden | ì—­ë°©í–¥ hidden] ì´ concatëœ ê²°ê³¼
        #
        # h_n: (num_layers*2, B, H)
        # - ë§ˆì§€ë§‰ layer ê¸°ì¤€:
        #   h_n[-2]: ì •ë°©í–¥ LSTMì˜ ë§ˆì§€ë§‰ timestep hidden
        #   h_n[-1]: ì—­ë°©í–¥ LSTMì˜ ë§ˆì§€ë§‰ timestep hidden
        #
        # âš ï¸ ì—¬ê¸°ì„œëŠ” many-to-one íšŒê·€ë¥¼ ìœ„í•´
        #     out[:, -1, :]ì„ ì‚¬ìš©í•œë‹¤.
        #
        # out[:, -1, :]ì€:
        # - ì •ë°©í–¥: ì‹œí€€ìŠ¤ ë(L)ê¹Œì§€ ì½ì€ ìš”ì•½
        # - ì—­ë°©í–¥: ì‹œí€€ìŠ¤ ë ìœ„ì¹˜ì—ì„œì˜ backward hidden
        #
        # (ì‹¤ì „ì—ì„œ ìì£¼ ì“°ì´ì§€ë§Œ, í•´ì„ì€ h_n ê¸°ë°˜ ë°©ì‹ë³´ë‹¤ ëœ ì§ê´€ì ì¼ ìˆ˜ ìˆìŒ)

        # last: (B, 2H)
        last = out[:, -1, :]

        # íšŒê·€ headë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ERA ì˜ˆì¸¡
        # y: (B, 1)
        y = self.head(last)
        return y

